{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f4ca0c",
   "metadata": {},
   "source": [
    "Блокнот с демонстрацией некоторых возможностей по обработке естественного языка, заготовка для лабораторной работы № 5 по курсу \"Методы искусственного интеллекта\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf362918",
   "metadata": {},
   "source": [
    "# 1. Лингвистический анализ\n",
    "\n",
    "Лингвистический анализ - это разбор текста, выделение слов, определение морфологических характеристик, структуры предложений и пр. Лингвистический анализ текста может быть как самостоятельной целью (если вы лингвист), так и выступать в качестве вспомогательного (и предварительного) этапа для каких-то прикладных задач обработки текста.\n",
    "\n",
    "Ознакомимся кратко с основными этапами лингвистического анализа, для чего воспользуемся библиотекой `natasha`. Эта библиотека предоставляет возможность проводит различные виды анализа - от графематического до синтаксического, обладает относительно низкими требованиями к ресурсам и достаточно высокой производительностью. Альтернативных решений несколько - одним из самых популярных моделей является библиотека Spacy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68603473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natasha\n",
    "import pymorphy2  # библиотека для морфологического анализа русского языка\n",
    "                  # является одной из зависимостей natasha\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14753538",
   "metadata": {},
   "source": [
    "Загрузим набор данных. Мы будем использовать подмножество набора новостей с сайта Lenta.ru (https://www.kaggle.com/datasets/yutkin/corpus-of-russian-news-articles-from-lenta). В наборе данных оставлены только новости на тему экономики, культуры и спорта.\n",
    "\n",
    "\"Усеченный\" набор следует скачать по адресу https://disk.yandex.ru/d/PmkxMoQjN7zHCg \n",
    "и распаковать таким образом, чтобы файл `lenta-subset.csv` оказался по следующему пути: `data/lenta`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b06bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>tags</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://lenta.ru/news/1999/10/04/tv/</td>\n",
       "      <td>Телеканалы станут вещать по единому тарифу</td>\n",
       "      <td>С 1 января 2000 года все телеканалы будут опла...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>Все</td>\n",
       "      <td>1999/10/04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://lenta.ru/news/1999/10/04/volkswagen/</td>\n",
       "      <td>Volkswagen выкупает остатки акций \"Шкоды\"</td>\n",
       "      <td>Германский автопромышленный концерн Volkswagen...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>Все</td>\n",
       "      <td>1999/10/04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://lenta.ru/news/1999/10/04/tumen/</td>\n",
       "      <td>Прибыль Тюменнефтегаза возросла в 10 раз</td>\n",
       "      <td>Нераспределенная прибыль ОАО \"Тюменнефтегаз\", ...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>Все</td>\n",
       "      <td>1999/10/04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://lenta.ru/news/1999/10/05/sprint/</td>\n",
       "      <td>Крупнейшее в истории слияние компаний происход...</td>\n",
       "      <td>Две крупнейших телекоммуникационных компании С...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>Все</td>\n",
       "      <td>1999/10/05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://lenta.ru/news/1999/10/05/volga/</td>\n",
       "      <td>ГАЗ получил четверть обещанного кредита</td>\n",
       "      <td>ОАО \"ГАЗ\" и Нижегородский банк Сбербанка Росси...</td>\n",
       "      <td>Экономика</td>\n",
       "      <td>Все</td>\n",
       "      <td>1999/10/05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197733</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/oleinik/</td>\n",
       "      <td>Российский боец UFC включен в Книгу рекордов Г...</td>\n",
       "      <td>Российский боец смешанного стиля (MMA) Алексей...</td>\n",
       "      <td>Спорт</td>\n",
       "      <td>Бокс и ММА</td>\n",
       "      <td>2018/12/15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197734</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/frank_myr/</td>\n",
       "      <td>Бывший чемпион UFC не выдержал кровопролития и...</td>\n",
       "      <td>Американский боец смешанного стиля (MMA) Фрэн...</td>\n",
       "      <td>Спорт</td>\n",
       "      <td>Бокс и ММА</td>\n",
       "      <td>2018/12/15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197735</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/mebel/</td>\n",
       "      <td>Моуринью сравнил футболистов с мебелью</td>\n",
       "      <td>Главный тренер «Манчестер Юнайтед» Жозе Моурин...</td>\n",
       "      <td>Спорт</td>\n",
       "      <td>Футбол</td>\n",
       "      <td>2018/12/15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197736</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/putinrap/</td>\n",
       "      <td>Путин предостерег от запретов рэп-концертов</td>\n",
       "      <td>Президент России Владимир Путин, выступая на з...</td>\n",
       "      <td>Культура</td>\n",
       "      <td>Музыка</td>\n",
       "      <td>2018/12/15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197737</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/gizin/</td>\n",
       "      <td>Падение горнолыжника на полной скорости попало...</td>\n",
       "      <td>Швейцарский горнолыжник Марк Гизин неудачно пр...</td>\n",
       "      <td>Спорт</td>\n",
       "      <td>Зимние виды</td>\n",
       "      <td>2018/12/15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197738 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0               https://lenta.ru/news/1999/10/04/tv/   \n",
       "1       https://lenta.ru/news/1999/10/04/volkswagen/   \n",
       "2            https://lenta.ru/news/1999/10/04/tumen/   \n",
       "3           https://lenta.ru/news/1999/10/05/sprint/   \n",
       "4            https://lenta.ru/news/1999/10/05/volga/   \n",
       "...                                              ...   \n",
       "197733     https://lenta.ru/news/2018/12/15/oleinik/   \n",
       "197734   https://lenta.ru/news/2018/12/15/frank_myr/   \n",
       "197735       https://lenta.ru/news/2018/12/15/mebel/   \n",
       "197736    https://lenta.ru/news/2018/12/15/putinrap/   \n",
       "197737       https://lenta.ru/news/2018/12/15/gizin/   \n",
       "\n",
       "                                                    title  \\\n",
       "0              Телеканалы станут вещать по единому тарифу   \n",
       "1               Volkswagen выкупает остатки акций \"Шкоды\"   \n",
       "2                Прибыль Тюменнефтегаза возросла в 10 раз   \n",
       "3       Крупнейшее в истории слияние компаний происход...   \n",
       "4                 ГАЗ получил четверть обещанного кредита   \n",
       "...                                                   ...   \n",
       "197733  Российский боец UFC включен в Книгу рекордов Г...   \n",
       "197734  Бывший чемпион UFC не выдержал кровопролития и...   \n",
       "197735             Моуринью сравнил футболистов с мебелью   \n",
       "197736        Путин предостерег от запретов рэп-концертов   \n",
       "197737  Падение горнолыжника на полной скорости попало...   \n",
       "\n",
       "                                                     text      topic  \\\n",
       "0       С 1 января 2000 года все телеканалы будут опла...  Экономика   \n",
       "1       Германский автопромышленный концерн Volkswagen...  Экономика   \n",
       "2       Нераспределенная прибыль ОАО \"Тюменнефтегаз\", ...  Экономика   \n",
       "3       Две крупнейших телекоммуникационных компании С...  Экономика   \n",
       "4       ОАО \"ГАЗ\" и Нижегородский банк Сбербанка Росси...  Экономика   \n",
       "...                                                   ...        ...   \n",
       "197733  Российский боец смешанного стиля (MMA) Алексей...      Спорт   \n",
       "197734   Американский боец смешанного стиля (MMA) Фрэн...      Спорт   \n",
       "197735  Главный тренер «Манчестер Юнайтед» Жозе Моурин...      Спорт   \n",
       "197736  Президент России Владимир Путин, выступая на з...   Культура   \n",
       "197737  Швейцарский горнолыжник Марк Гизин неудачно пр...      Спорт   \n",
       "\n",
       "               tags        date  \n",
       "0               Все  1999/10/04  \n",
       "1               Все  1999/10/04  \n",
       "2               Все  1999/10/04  \n",
       "3               Все  1999/10/05  \n",
       "4               Все  1999/10/05  \n",
       "...             ...         ...  \n",
       "197733   Бокс и ММА  2018/12/15  \n",
       "197734   Бокс и ММА  2018/12/15  \n",
       "197735       Футбол  2018/12/15  \n",
       "197736       Музыка  2018/12/15  \n",
       "197737  Зимние виды  2018/12/15  \n",
       "\n",
       "[197738 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/lenta/lenta-subset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb1a01",
   "metadata": {},
   "source": [
    "## 1.1 Токенизация (графематический анализ)\n",
    "\n",
    "Последовательность обработки документа с помощью библиотеки `natasha` строится вокруг концепции документ (класс `Doc`). Изначально документ представляет собой простой текст, но по мере применения различных алгоритмов лингвистического анализа он \"обрастает\" новыми деталями, касающимися того или иного аспекта.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d98968",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = natasha.Doc(df.iloc[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33daca78",
   "metadata": {},
   "source": [
    "В библиотеке `natasha` за токенизацию отвечает компонент `Segmenter`. Создадим экземпляр компонента сегментации и применим его к тексту:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f2cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = natasha.Segmenter()\n",
    "doc.segment(segmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85120d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(text='С 1 января 2000 года все телеканалы будут оплачив..., tokens=[...], sents=[...])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf71a3",
   "metadata": {},
   "source": [
    "Мы видим, что поле `text` документа по-прежнему хранит исходный текст, но помимо него у экземпляра `Doc` появились свойства `tokens` и `sents`, содержащие токены и предложения соответственно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4032444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSent(stop=104, text='С 1 января 2000 года все телеканалы будут оплачив..., tokens=[...]),\n",
       " DocSent(start=105, stop=299, text='Как сообщило министерство Российской Федерации по..., tokens=[...]),\n",
       " DocSent(start=300, stop=485, text='В настоящее время общенациональные телеканалы (ОР..., tokens=[...]),\n",
       " DocSent(start=486, stop=596, text='С 1 января для ОРТ, НТВ и ВГТРК тарифы будут увел..., tokens=[...])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b69aea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=1, text='С'),\n",
       " DocToken(start=2, stop=3, text='1'),\n",
       " DocToken(start=4, stop=10, text='января'),\n",
       " DocToken(start=11, stop=15, text='2000'),\n",
       " DocToken(start=16, stop=20, text='года')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875bd25",
   "metadata": {},
   "source": [
    "Следует иметь в виду, что свойство `tokens` содержит все токены подряд (без разбивки на предложения), если необходимо получать токены по предложениям, то это можно делать через свойство `sents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5ae9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=1, text='С'),\n",
       " DocToken(start=2, stop=3, text='1'),\n",
       " DocToken(start=4, stop=10, text='января'),\n",
       " DocToken(start=11, stop=15, text='2000'),\n",
       " DocToken(start=16, stop=20, text='года'),\n",
       " DocToken(start=21, stop=24, text='все'),\n",
       " DocToken(start=25, stop=35, text='телеканалы'),\n",
       " DocToken(start=36, stop=41, text='будут'),\n",
       " DocToken(start=42, stop=52, text='оплачивать'),\n",
       " DocToken(start=53, stop=59, text='услуги'),\n",
       " DocToken(start=60, stop=68, text='передачи'),\n",
       " DocToken(start=69, stop=85, text='телерадиосигнала'),\n",
       " DocToken(start=86, stop=88, text='по'),\n",
       " DocToken(start=89, stop=96, text='единому'),\n",
       " DocToken(start=97, stop=103, text='тарифу'),\n",
       " DocToken(start=103, stop=104, text='.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents[0].tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78559275",
   "metadata": {},
   "source": [
    "Список токенов состоит из экземпляров класса `DocToken`, если (например, при работе с каким-то внешним инструментом) нужно преобразовать его в список строк, то можно сделать это следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2b8406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['С',\n",
       " '1',\n",
       " 'января',\n",
       " '2000',\n",
       " 'года',\n",
       " 'все',\n",
       " 'телеканалы',\n",
       " 'будут',\n",
       " 'оплачивать',\n",
       " 'услуги',\n",
       " 'передачи',\n",
       " 'телерадиосигнала',\n",
       " 'по',\n",
       " 'единому',\n",
       " 'тарифу',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.text for x in doc.sents[0].tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc489c0",
   "metadata": {},
   "source": [
    "## 1.2 Морфологический анализ\n",
    "\n",
    "На этапе морфологического анализа токены сопровождаются морфологическими тегами (часть речи, род, падеж и пр.). При этом, по форме слова не всегда однозначно понятно к какой части речи она может относиться. Морфологический анализ связан с разрешением частеречной омонимии, для чего используется контекст слова. В ходе морфологического анализа `natasha` опирается на библиотеку `pymorphy2`, которая для каждого слова выдает все возможные теги, `natasha` же осуществляет разрешение неоднозначностей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172993f",
   "metadata": {},
   "source": [
    "Сначала воспользуемся `pymorphy2` для получения всех возможных тегов для слова \"мыла\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db0a610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='мыло', score=0.333333, methods_stack=((DictionaryAnalyzer(), 'мыла', 54, 1),)),\n",
       " Parse(word='мыла', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='мыть', score=0.333333, methods_stack=((DictionaryAnalyzer(), 'мыла', 2074, 8),)),\n",
       " Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='мыло', score=0.166666, methods_stack=((DictionaryAnalyzer(), 'мыла', 54, 6),)),\n",
       " Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='мыло', score=0.166666, methods_stack=((DictionaryAnalyzer(), 'мыла', 54, 9),))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse('мыла')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b5670",
   "metadata": {},
   "source": [
    "Действительно, это может быть и существительное \"мыло\" в родительном падеже (а также в двух других падежных формах), и глагол \"мыть\" в женском роде прошедшем времени. Можно обратить внимание, что для каждого варианта тега `pymorphy2` возвращает `score` - по сути, это нормализованная (между разными тегами) частота того, как часто используется именно эта форма в части OpenCorpora со снятой частеречной омонимией. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d27f69",
   "metadata": {},
   "source": [
    "Попробуем обработать ту же фразу с помощью `natasha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec63774",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguity_example = natasha.Doc('Мама мыла раму.')\n",
    "ambiguity_example.segment(segmenter)\n",
    "\n",
    "# Модель, обеспечивающая, в том числе, разрешение\n",
    "# частеречной омонимии\n",
    "emb = natasha.NewsEmbedding()\n",
    "morph_tagger = natasha.NewsMorphTagger(emb)\n",
    "\n",
    "ambiguity_example.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deaaf0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=4, text='Мама', pos='NOUN', feats=<Anim,Nom,Fem,Sing>),\n",
       " DocToken(start=5, stop=9, text='мыла', pos='VERB', feats=<Imp,Fem,Ind,Sing,Past,Fin,Act>),\n",
       " DocToken(start=10, stop=14, text='раму', pos='NOUN', feats=<Inan,Acc,Fem,Sing>),\n",
       " DocToken(start=14, stop=15, text='.', pos='PUNCT')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ambiguity_example.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de003d97",
   "metadata": {},
   "source": [
    "В результате применения метода `tag_morph()` каждый токен был снабжен морфологической информацией - поле `pos` (part-of-speech) содержит часть речи, к которой был отнесен токен, а `feats` - набор граммем, характеризующих форму слова. При этом - обратите внимание! - тег для каждого слова только один, `natasha` провела разрешение противоречий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7356ec",
   "metadata": {},
   "source": [
    "Проставим тэги для первой новости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76781f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b5dc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=1, text='С', pos='ADP'),\n",
       " DocToken(start=2, stop=3, text='1', pos='ADJ'),\n",
       " DocToken(start=4, stop=10, text='января', pos='NOUN', feats=<Inan,Gen,Masc,Sing>),\n",
       " DocToken(start=11, stop=15, text='2000', pos='ADJ'),\n",
       " DocToken(start=16, stop=20, text='года', pos='NOUN', feats=<Inan,Gen,Masc,Sing>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f68d5",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "\n",
    "Лемматизация - приведение слова к начальной форме. Для этого, очевидно, частеречная омонимия должна быть уже разрешена, соответственно, лемматизация опирается на результат морфологического анализа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0485674",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_vocab = natasha.MorphVocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776371b6",
   "metadata": {},
   "source": [
    "Лемматизация применяется не ко всему документу, а к заданным токенам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23a53d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58793b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=1, text='С', pos='ADP', lemma='с'),\n",
       " DocToken(start=2, stop=3, text='1', pos='ADJ', lemma='1'),\n",
       " DocToken(start=4, stop=10, text='января', pos='NOUN', feats=<Inan,Gen,Masc,Sing>, lemma='январь'),\n",
       " DocToken(start=11, stop=15, text='2000', pos='ADJ', lemma='2000'),\n",
       " DocToken(start=16, stop=20, text='года', pos='NOUN', feats=<Inan,Gen,Masc,Sing>, lemma='год')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5b8ad",
   "metadata": {},
   "source": [
    "В результате токены снабжаются дополнительным свойством - `lemma`, содержащим начальную форму слова."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff00a55",
   "metadata": {},
   "source": [
    "## 1.3 Синтаксический разбор\n",
    "\n",
    "В ходе синтаксического разбора строится дерево зависимостей между словами в каждом предложении. В библиотеке `natasha` синтаксический разбор реализуется классом `NewsSyntaxParser`. Экземпляр этого класса применяется к документу. При этом документ должен быть сегментирован (разбит на токены), но морфологический анализ проводить не обязательно (компонент синтаксического разбора не опирается на морфологические тэги и леммы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b5eb1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_parser = natasha.NewsSyntaxParser(emb)\n",
    "doc.parse_syntax(syntax_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee92e9f",
   "metadata": {},
   "source": [
    "Отобразить дерево зависимостей графически можно следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5738da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ┌► С                case\n",
      "  ┌►┌─┌─└─ 1                obl\n",
      "  │ │ └──► января           flat\n",
      "  │ │   ┌► 2000             amod\n",
      "  │ └──►└─ года             nmod\n",
      "  │     ┌► все              det\n",
      "  │   ┌►└─ телеканалы       nsubj\n",
      "  │   │ ┌► будут            aux\n",
      "┌─└───└─└─ оплачивать       \n",
      "│     └►┌─ услуги           obj\n",
      "│     ┌─└► передачи         nmod\n",
      "│   ┌─└──► телерадиосигнала iobj\n",
      "│   │ ┌──► по               case\n",
      "│   │ │ ┌► единому          amod\n",
      "│   └►└─└─ тарифу           obl\n",
      "└────────► .                punct\n"
     ]
    }
   ],
   "source": [
    "doc.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b890332",
   "metadata": {},
   "source": [
    "Однако, используя свойство `syntax`, можно написать код, анализирующий зависимости, и, например, извлекающий основу предложения (подлежащее и сказуемое):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b7491a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('телеканалы', 'будут оплачивать')\n",
      "('постановление', 'является')\n",
      "('телеканалы', 'оплачивают')\n",
      "('тарифы', 'будут увеличены')\n"
     ]
    }
   ],
   "source": [
    "def extract_basis(syntactic_tree):\n",
    "    # id2token = {x.id: x for x in syntactic_tree.tokens}\n",
    "    # Найти главное слово - сказуемое\n",
    "    root = [x for x in syntactic_tree.tokens if x.rel == 'root'][0]\n",
    "    # Проверить, есть ли у него какие-то модификаторы (aux)\n",
    "    aux = [x for x in syntactic_tree.tokens if x.head_id == root.id and (x.rel=='aux' or x.rel=='aux:pass')]\n",
    "    # Найти подлежащее\n",
    "    subject = [x for x in syntactic_tree.tokens if x.head_id == root.id and (x.rel == 'nsubj' or x.rel=='nsubj:pass')]\n",
    "    return subject[0].text, aux[0].text + ' ' + root.text if aux else root.text  \n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(extract_basis(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bfed44",
   "metadata": {},
   "source": [
    "Конечно, эта функция далека от той, что можно использовать на практике (например, она совершенно не учитывает сложных предложений, у которых может быть несколько основ), но общие идеи можно использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b8f16",
   "metadata": {},
   "source": [
    "## 1.4 Именованные сущности\n",
    "\n",
    "К именованным сущностям относятся имена людей, географических объектов, компаний и т.д. \n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448f938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f14d108b",
   "metadata": {},
   "source": [
    "# 2. Векторная модель документа, мешок слов и классификация\n",
    "\n",
    "Построим простой тематический классификатор новостей, основанный на векторной модели документа. Для этого нужно:\n",
    "\n",
    "1. Выделить множество признаков, которое будет соответствовать множеству всех слов, которые встречаются во всех новостных заметках (возможно, исключив самые частотные и самые редко встречающиеся).\n",
    "2. Преобразовать каждую новостную заметку в вектор, в котором ненулевые значения будут соответствовать признакам-словам, которые присутствуют в заметке. Само ненулевое значение может быть получено по-разному - частота слова, TF-IDF, и пр.\n",
    "3. Определить целевую переменную (метку классификации).\n",
    "4. Обучить модель (логистической регрессии).\n",
    "\n",
    "Все эти шаги легко проделать с помощью библиотеки `scikit-learn`. Так, для первых двух этапов в ней предусмотрен набор \"векторизаторов\" (`CountVectorizer`, `TfIdfVectorizer`), а для обучения логистической регрессии `linear_model.LogisticRegression`. Кроме того, библиотека предоставляет набор инструментов, облегчающих типовые задачи машинного обучения: разбиение на обучающую и тестовую выборки, кросс-валидация и пр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce3decf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e170d",
   "metadata": {},
   "source": [
    "Поставим задачу классификации как классификация новостей, относящихся к теме \"Культура\". Соответственно, уберем из набора данных все лишние поля и сформируем целевую метку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0295e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = (df.topic == 'Культура').astype(np.int8)\n",
    "df = df[['text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77746de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatter\\AppData\\Local\\Temp\\ipykernel_18612\\1379821321.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfa28e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>С 1 января 2000 года все телеканалы будут опла...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Германский автопромышленный концерн Volkswagen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Нераспределенная прибыль ОАО \"Тюменнефтегаз\", ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Две крупнейших телекоммуникационных компании С...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ОАО \"ГАЗ\" и Нижегородский банк Сбербанка Росси...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197733</th>\n",
       "      <td>Российский боец смешанного стиля (MMA) Алексей...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197734</th>\n",
       "      <td>Американский боец смешанного стиля (MMA) Фрэн...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197735</th>\n",
       "      <td>Главный тренер «Манчестер Юнайтед» Жозе Моурин...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197736</th>\n",
       "      <td>Президент России Владимир Путин, выступая на з...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197737</th>\n",
       "      <td>Швейцарский горнолыжник Марк Гизин неудачно пр...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197737 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  target\n",
       "0       С 1 января 2000 года все телеканалы будут опла...       0\n",
       "1       Германский автопромышленный концерн Volkswagen...       0\n",
       "2       Нераспределенная прибыль ОАО \"Тюменнефтегаз\", ...       0\n",
       "3       Две крупнейших телекоммуникационных компании С...       0\n",
       "4       ОАО \"ГАЗ\" и Нижегородский банк Сбербанка Росси...       0\n",
       "...                                                   ...     ...\n",
       "197733  Российский боец смешанного стиля (MMA) Алексей...       0\n",
       "197734   Американский боец смешанного стиля (MMA) Фрэн...       0\n",
       "197735  Главный тренер «Манчестер Юнайтед» Жозе Моурин...       0\n",
       "197736  Президент России Владимир Путин, выступая на з...       1\n",
       "197737  Швейцарский горнолыжник Марк Гизин неудачно пр...       0\n",
       "\n",
       "[197737 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17f936bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.7,    # Делать признаками слова, которые содержатся в не более, чем заданной доле документов\n",
    "    min_df=10      # Делать признаки из слов, которые содержатся, по крайней мере, в заданном количестве документов\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f92bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6700726a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<197737x106721 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 24058646 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6337238",
   "metadata": {},
   "source": [
    "Полученная матрица является разреженной матрицей. Видно, что в результате векторизации был сделан 106721 признак (каждый соответствует слову). Если бы матрица не была разреженной, то в ней было бы больше ста миллиардов значений (что, конечно, не поместилось бы в память), однако ненулевых значений в ней около 24 млн., и это вполне разумное количество."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc00c6",
   "metadata": {},
   "source": [
    "Для обучения логистической регрессии (да и большиства моделей) следует нормализовать данные. Для этого в `sklearn` есть несколько методов. Самый популярный выбор - `StandardScaler`, который пытается привести распределение каждого атрибута к центрированному в 0 нормальному, для этого из значения атрибута вычитается среднее арифметическое по столбцу и делится на стандартное отклонение. Но мы поступим немного по-другому. Нормализуем каждую строку, перейдя к относительной частоте слова. Для этого разделим каждую строку матрицы на сумму соответствующей строки (применив так называемое сглаживание Лапласа, чтобы избежать деления на 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ed886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / (X.sum(axis=1) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504ac0b",
   "metadata": {},
   "source": [
    "Разделим имеющиеся данные на обучающее и тестовое множества:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48e23a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df.target, test_size=0.2, random_state=42, stratify=df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e77603",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e376c922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LogisticRegression()\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449c662",
   "metadata": {},
   "source": [
    "Наконец, оценим качество модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed3f7cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9419439668251239\n",
      "ROC_AUC: 0.9939112907002375\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, lm.predict(X_test)))\n",
    "print('ROC_AUC:', roc_auc_score(y_test, lm.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003292f",
   "metadata": {},
   "source": [
    "Получилось вполне неплохо! Такое высокое качество объясняется несколькими причинами:\n",
    "\n",
    "1. Всё-таки, это очень простая задача. Во-первых, мы довольно сильно ограничили количество классов как в обучающей, так и в тестовой выборках. Во-вторых, тематическая классификация в постановке \"Культура\"-против-остальных действительно довольно неплохо может решаться просто по вхождению некоторых слов: \"балет\", \"концерт\" и пр.\n",
    "2. Некоторые шаги выше для простоты были сделаны не совсем \"честно\". Например, то, что векторизатор обучался на всём множестве документов (включая тестовое множество). Очевидно, это не совсем соответствует сценарию реального применения, когда векторизатор обучается только на обучающем множестве, а вновь поступающие документы просто им обрабатываются и новая лексика, которая не была известна на момент обучения модели, будет просто проигнорирована.\n",
    "\n",
    "Кроме того, есть еще ряд путей для возможного улучшения:\n",
    "\n",
    "1. Более надежная оценка модели достигается с применением кросс-валидации.\n",
    "2. Можно попробовать модифицировать процесс выделения признаков, добавив лемматизацию и более точную токенизацию, используя библиотеку `natasha`.\n",
    "3. Можно попробовать получать признаки, взвешенные по TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4efe4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1.).reset_index(drop=True)\n",
    "df_test = df[:1000].copy()\n",
    "df_train = df[1000:].copy().reset_index(drop=True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86c9fec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.7,    # Делать признаками слова, которые содержатся в не более, чем заданной доле документов\n",
    "    min_df=10      # Делать признаки из слов, которые содержатся, по крайней мере, в заданном количестве документов\n",
    ")\n",
    "X_train = vectorizer.fit_transform(df_train.text)\n",
    "X_train = X_train / (X_train.sum(axis=1) + 1)\n",
    "lm = LogisticRegression()\n",
    "lm.fit(X_train, df_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4029b578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.943\n",
      "ROC_AUC: 0.9963486388763165\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(df_test.text)\n",
    "X_test = X_test / (X_test.sum(axis=1) + 1)\n",
    "print('Accuracy:', accuracy_score(df_test.target, lm.predict(X_test)))\n",
    "print('ROC_AUC:', roc_auc_score(df_test.target, lm.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698bb75",
   "metadata": {},
   "source": [
    "Попробуем обучить модель с более \"интеллектуальным\" подходом к подготовке признаков - приведением к нормальной форме (лемме) и выбрасыванием всех слов, кроме существительных и глаголов. К сожалению, морфологический разбор требует времени, поэтому применение подобных преобразований ко всему набору данных оказывается довольно длительным процессом (на практике использовать можно, но в целях обучения - нецелесообразно). Поэтому сократим обучающую часть - будем использовать для обучения лишь 10 тыс. сообщений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6927ee04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmartTokenizer:\n",
    "    \"\"\"Класс для выделения токенов.\n",
    "    \n",
    "    Использует библиотеку natasha для токенизации и лемматизации,\n",
    "    оставляет только существительные и глаголы.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        emb = natasha.NewsEmbedding()\n",
    "        self.segmenter = natasha.Segmenter()\n",
    "        self.morph_tagger = natasha.NewsMorphTagger(emb)\n",
    "        self.morph_vocab = natasha.MorphVocab()\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        doc = natasha.Doc(text)\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        tokens = []\n",
    "        for token in doc.tokens:\n",
    "            if token.pos in ['NOUN', 'VERB']:\n",
    "                token.lemmatize(morph_vocab)\n",
    "                tokens.append(token.lemma)\n",
    "        return tokens\n",
    "\n",
    "improved_vectorizer = CountVectorizer(\n",
    "    tokenizer=SmartTokenizer(),\n",
    "    token_pattern=None,\n",
    "    max_df=0.7,    # Делать признаками слова, которые содержатся в не более, чем заданной доле документов\n",
    "    min_df=10      # Делать признаки из слов, которые содержатся, по крайней мере, в заданном количестве документов\n",
    ")\n",
    "\n",
    "# Подмножество обучающего набора\n",
    "df_train_small = df_train[:10000]\n",
    "\n",
    "X_train = improved_vectorizer.fit_transform(df_train_small.text)\n",
    "X_train = X_train / (X_train.sum(axis=1) + 1)\n",
    "lm = LogisticRegression()\n",
    "lm.fit(X_train, df_train_small.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95288bb",
   "metadata": {},
   "source": [
    "Несмотря на то, что мы ограничили обучающее множество всего 10 тыс. новостных сообщений, ячейка выполнялась существенно дольше, чем подготовка почти 200 тыс. сообщений с помощью простой токенизации (около 3 минут на моем компьютере)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e179fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.863\n",
      "ROC_AUC: 0.9975894914030279\n"
     ]
    }
   ],
   "source": [
    "X_test = improved_vectorizer.transform(df_test.text)\n",
    "X_test = X_test / (X_test.sum(axis=1) + 1)\n",
    "print('Accuracy:', accuracy_score(df_test.target, lm.predict(X_test)))\n",
    "print('ROC_AUC:', roc_auc_score(df_test.target, lm.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b3a14",
   "metadata": {},
   "source": [
    "Результат довольно противоречивый. Во-первых, разные метрики по-разному \"отреагировали\" на изменение. Точность (accuracy) существенно уменьшилась, однако ROC AUC почти не изменился. По-прежнему высокий ROC AUC говорит о том, что модель неплохо ранжирует результаты (то есть, выдает, как правило, большие значения вероятности для новостных сообщений, действительно относящихся к теме Культура). Низкая же точность свидетельствует о том, что модель стала хуже калиброванной. То есть, порог 0.5, который используется по умолчанию, чтобы переводить вероятности, появляющиеся на выходе модели, в нули и единицы, не очень хорош. Попробуем другие пороги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "730692ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (thr=0.2): 0.7580\n",
      "Accuracy (thr=0.3): 0.9830\n",
      "Accuracy (thr=0.4): 0.9410\n",
      "Accuracy (thr=0.5): 0.8630\n",
      "Accuracy (thr=0.6): 0.8140\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "    print(f'Accuracy (thr={thr}): {accuracy_score(df_test.target, lm.predict_proba(X_test)[:, 1] > thr):.4f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c626c",
   "metadata": {},
   "source": [
    "Видно, что при пороге классификации около 0.3 точность достигает аж 0.97! Правда, идея использования тестового множества для подбора порога - так себе. Посмотрим что с обучающим множеством:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a16f71bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (thr=0.2): 0.7516\n",
      "Accuracy (thr=0.3): 0.9781\n",
      "Accuracy (thr=0.4): 0.9456\n",
      "Accuracy (thr=0.5): 0.8661\n",
      "Accuracy (thr=0.6): 0.8007\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "    print(f'Accuracy (thr={thr}): {accuracy_score(df_train_small.target, lm.predict_proba(X_train)[:, 1] > thr):.4f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadc39a",
   "metadata": {},
   "source": [
    "Тот же эффект. Таким образом, мы можем обоснованно выбрать порог классификации в 0.3 на обучающем множестве и добиться достаточно высокой точности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "558a3aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x6402 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 605042 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b869a79",
   "metadata": {},
   "source": [
    "Обратите внимание, что из-за применения лемматизации матрица признаков оказалась существенно  менее широкой и гораздо более \"плотной\". Кроме того, для обучения достаточно хорошего классификатора хватило всего 10 тыс. сообщений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645e8d0",
   "metadata": {},
   "source": [
    "## Интерпретация модели\n",
    "\n",
    "Одним из достоинств линейных моделей является их интерпретируемость. Действительно, математически наша модель записывается в виде:\n",
    "\n",
    "$$\n",
    "y=\\sigma(w_1x_1 + ... w_nx_n)\n",
    "$$\n",
    "\n",
    "Где $x_1$ - это частота слова в документе. Соответственно, положительные коэффициенты $w_1$ соответствуют тем словам, присутствие которых повышает вероятность положительной классификации объекта, а отрицательные - тем, которые понижают.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f859efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [''] * len(improved_vectorizer.vocabulary_)\n",
    "for k, v in improved_vectorizer.vocabulary_.items():\n",
    "    features[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a23c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = pd.DataFrame({'word': features,\n",
    "                    'effect': lm.coef_.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4447892a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>фильм</td>\n",
       "      <td>14.210825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>картина</td>\n",
       "      <td>8.264702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>режиссер</td>\n",
       "      <td>5.570922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>альбом</td>\n",
       "      <td>5.291805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>роль</td>\n",
       "      <td>5.204574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>актер</td>\n",
       "      <td>5.174695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>группа</td>\n",
       "      <td>5.003640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>театр</td>\n",
       "      <td>4.592963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4055</th>\n",
       "      <td>премия</td>\n",
       "      <td>4.356246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>музей</td>\n",
       "      <td>4.014702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>музыкант</td>\n",
       "      <td>3.981445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>работа</td>\n",
       "      <td>3.955690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2176</th>\n",
       "      <td>концерт</td>\n",
       "      <td>3.926186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3552</th>\n",
       "      <td>песня</td>\n",
       "      <td>3.796807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>фестиваль</td>\n",
       "      <td>3.573830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>книга</td>\n",
       "      <td>3.528002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4906</th>\n",
       "      <td>сериал</td>\n",
       "      <td>2.938646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>искусство</td>\n",
       "      <td>2.878810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>имя</td>\n",
       "      <td>2.855228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>художник</td>\n",
       "      <td>2.830672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word     effect\n",
       "5916      фильм  14.210825\n",
       "1955    картина   8.264702\n",
       "4578   режиссер   5.570922\n",
       "140      альбом   5.291805\n",
       "4694       роль   5.204574\n",
       "100       актер   5.174695\n",
       "1071     группа   5.003640\n",
       "5450      театр   4.592963\n",
       "4055     премия   4.356246\n",
       "2702      музей   4.014702\n",
       "2705   музыкант   3.981445\n",
       "4391     работа   3.955690\n",
       "2176    концерт   3.926186\n",
       "3552      песня   3.796807\n",
       "5899  фестиваль   3.573830\n",
       "2062      книга   3.528002\n",
       "4906     сериал   2.938646\n",
       "1849  искусство   2.878810\n",
       "1775        имя   2.855228\n",
       "6096   художник   2.830672"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.sort_values(['effect'], ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdc8b8",
   "metadata": {},
   "source": [
    "# 3 Эмбеддинги слов (вложения, embeddings)\n",
    "\n",
    "В библиотеке `natasha` есть также набор эмбеддингов слов для русского языка. На самом деле, мы их уже раньше использовали (не напрямую, но передавали в качестве параметров классам, осуществляющим разные операции лингвистического анализа). Эмбеддинги слов хранятся в экземпляре `emb` класса `natasha.NewsEmbedding`. Простейший способ получения эмбеддинга слова: `emb['слово']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebc38589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.55429476e-01,  5.89054311e-03,  1.57977819e-01,  1.82602152e-01,\n",
       "       -2.25324184e-01, -2.96944350e-01,  3.48798364e-01, -6.22543991e-02,\n",
       "       -4.75153625e-01, -4.50215518e-01,  6.82648644e-02, -1.88942790e-01,\n",
       "       -2.57037114e-02,  3.50426584e-01, -3.09493065e-01,  3.69875431e-01,\n",
       "       -3.96766871e-01,  3.58251959e-01, -1.06716901e-01,  2.17676908e-01,\n",
       "       -8.67002904e-02, -1.70834616e-01, -6.49302825e-02, -4.35781889e-02,\n",
       "       -1.85484979e-02, -6.33450031e-01,  3.89721841e-01, -2.16791287e-01,\n",
       "       -4.60259497e-01,  2.69960612e-01, -4.14235801e-01, -1.01559913e+00,\n",
       "        3.47699993e-03,  3.28725785e-01,  1.26028627e-01,  7.60548934e-02,\n",
       "        3.50357533e-01, -5.79251111e-01,  9.40726846e-02, -1.22430496e-01,\n",
       "       -2.66330205e-02,  7.68245697e-01, -3.23462449e-02, -3.32074314e-01,\n",
       "       -1.42027378e-01, -3.43501210e-01, -3.53978842e-01,  3.05407830e-02,\n",
       "        3.05212557e-01, -1.34403229e-01,  6.84357047e-01, -3.43205690e-01,\n",
       "        3.83923985e-02, -3.09770435e-01, -1.76839903e-01, -2.04010710e-01,\n",
       "        2.01983407e-01,  6.52837753e-01,  1.43933594e-01, -4.14080471e-02,\n",
       "        2.99581140e-01,  2.28598908e-01,  2.86565781e-01, -3.28909233e-02,\n",
       "        5.16278386e-01, -1.28400177e-01, -2.20988505e-02,  1.58519506e-01,\n",
       "        9.93166864e-02, -3.33152205e-01,  6.38782144e-01,  7.10417032e-02,\n",
       "       -8.10740069e-02,  8.17873716e-01, -4.04118374e-02, -2.62528807e-01,\n",
       "       -3.84427607e-02,  5.49760163e-01,  2.06742287e-01, -2.05440283e-01,\n",
       "       -1.96745381e-01, -1.46580502e-01, -3.23234200e-01,  3.14118862e-01,\n",
       "        2.85320934e-02,  3.77618045e-01, -6.40179873e-01, -2.48229414e-01,\n",
       "        1.18721880e-01, -8.65737498e-02,  5.71308911e-01,  1.07056282e-01,\n",
       "       -1.51580736e-01,  3.64351809e-01,  5.34143269e-01, -5.44755757e-01,\n",
       "       -1.94899902e-01,  7.46047720e-02,  4.93412048e-01, -4.59763497e-01,\n",
       "        5.71132898e-01, -3.33417118e-01,  3.76280636e-01, -5.79480052e-01,\n",
       "       -3.51704136e-02,  6.88241839e-01, -3.51206720e-01,  8.55212510e-02,\n",
       "       -7.10186958e-01, -4.01605994e-01,  3.93215597e-01,  3.11232448e-01,\n",
       "        4.04377878e-02, -2.12554395e-01,  6.20170653e-01,  4.77294713e-01,\n",
       "       -4.41487670e-01, -1.53186113e-01,  1.02707483e-01, -4.90125328e-01,\n",
       "        1.01286910e-01, -1.10924989e-02,  2.94891372e-02,  7.01225996e-01,\n",
       "       -2.51385748e-01, -9.20556262e-02, -9.14207697e-02, -6.74451962e-02,\n",
       "        2.02474356e-01,  1.99010968e-01, -1.18385851e-01,  5.49648464e-01,\n",
       "       -2.85186321e-01, -5.54305255e-01,  3.14947844e-01, -3.92118126e-01,\n",
       "       -1.92474052e-01, -1.11701921e-01, -6.72321379e-01, -3.43576133e-01,\n",
       "       -9.63334553e-03, -5.05883038e-01, -2.48802587e-01, -3.82802874e-01,\n",
       "        5.38600236e-02, -3.35214198e-01, -5.02326787e-01, -2.03828007e-01,\n",
       "        5.68976887e-02,  2.32490733e-01, -3.10611039e-01, -3.19361240e-01,\n",
       "        4.17561941e-02, -8.71700123e-02,  3.67845565e-01, -1.79859415e-01,\n",
       "       -7.15529397e-02,  1.17080718e-01, -5.81188761e-02, -2.27808848e-01,\n",
       "       -5.45225501e-01,  1.90417156e-01,  5.38773984e-02,  3.70366067e-01,\n",
       "        7.60966390e-02, -2.10585475e-01,  3.32648844e-01, -2.02435508e-01,\n",
       "        2.19890952e-01, -1.37495488e-01, -3.86399001e-01,  1.95725188e-02,\n",
       "        2.93809086e-01,  4.13037688e-01,  6.00573659e-01,  3.34273070e-01,\n",
       "        3.78607482e-01, -5.57399333e-01,  6.98299855e-02,  2.21008405e-01,\n",
       "        2.79969331e-02, -9.32590187e-01,  2.14914426e-01, -3.15415621e-01,\n",
       "        1.40197694e-01, -1.22565903e-01,  4.70766723e-01, -2.34497160e-01,\n",
       "        2.42874846e-02, -2.01442223e-02,  3.74472030e-02, -4.73277527e-04,\n",
       "        6.62438929e-01,  5.84455192e-01, -1.20954588e-02, -8.75058472e-02,\n",
       "        5.76973617e-01, -4.85449642e-01,  9.55752060e-02, -4.18336183e-01,\n",
       "       -7.32178390e-02,  2.46201172e-01,  1.28158426e+00, -3.16103429e-01,\n",
       "       -1.24979556e-01,  1.09421030e-01,  7.82902613e-02,  2.93608546e-01,\n",
       "       -1.20467618e-01, -2.41415441e-01,  1.51659369e-01,  3.79214548e-02,\n",
       "        3.06108087e-01, -6.41471893e-02, -4.35429215e-01,  2.06730396e-01,\n",
       "       -2.03913748e-01,  3.89085829e-01, -1.34151783e-02,  2.70148665e-01,\n",
       "        6.36630952e-02,  6.67431772e-01,  7.29049683e-01,  1.78423718e-01,\n",
       "       -2.91641541e-02,  4.51886021e-02, -2.07249686e-01, -2.00737283e-01,\n",
       "       -1.67889893e-01,  1.87974110e-01,  7.92541623e-01,  1.93429306e-01,\n",
       "        1.59006044e-01,  3.27419996e-01, -2.44676918e-01, -2.79762834e-01,\n",
       "        1.55361339e-01,  1.99094210e-02,  9.10742953e-02,  1.68040264e-02,\n",
       "       -3.38258944e-03, -1.44980904e-02,  4.33978498e-01,  2.84450442e-01,\n",
       "       -4.08669412e-01, -3.40028286e-01,  4.21045780e-01,  1.30362308e+00,\n",
       "        3.64003032e-01, -3.08113918e-02,  6.54399514e-01,  1.35665730e-01,\n",
       "       -3.73499781e-01, -9.81447458e-01, -2.79791653e-01, -7.35620618e-01,\n",
       "        6.55466393e-02,  1.51045993e-01,  2.96576113e-01, -6.41526952e-02,\n",
       "        2.08565995e-01, -7.46638715e-01, -1.07642196e-01,  8.98803547e-02,\n",
       "        2.46050674e-03,  1.80621237e-01, -3.44611675e-01,  1.29254296e-01,\n",
       "       -1.58809900e-01, -2.00735912e-01,  1.44807950e-01,  6.96170449e-01,\n",
       "       -1.68500375e-02,  1.16964594e-01,  4.08243865e-01,  4.78250057e-01,\n",
       "        2.73599237e-01,  1.59908131e-01,  1.00008525e-01, -1.75547034e-01,\n",
       "       -5.91146760e-02, -1.65788949e-01,  2.55977422e-01, -5.73939204e-01,\n",
       "       -1.20024756e-01, -1.02393627e-01,  7.94436753e-01, -1.22512557e-01,\n",
       "       -6.58337697e-02, -3.55146863e-02, -2.08910197e-01, -4.09074605e-01,\n",
       "        3.74932699e-02,  3.68406177e-01, -6.58255816e-01, -2.49600485e-01,\n",
       "       -8.15062448e-02, -6.09179437e-01,  5.95361814e-02, -3.09524477e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['слово']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a570b6",
   "metadata": {},
   "source": [
    "Попробуем использовать эмбеддинги для поиска близких по значению слов (нужно помнить, что это не синонимы в полной мере) и для рассуждения по аналогии. Для этого реализуем пару функций, осуществляющих поиск слов, ближайших к заданному вектору: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb01a62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['море',\n",
       " 'средиземном',\n",
       " 'моря',\n",
       " 'черном',\n",
       " 'черное',\n",
       " 'баренцевом',\n",
       " 'берегов',\n",
       " 'средиземное',\n",
       " 'эгейском',\n",
       " 'южно-китайское']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def squared_euclidean_sim(x, y):\n",
    "    return -np.sum((x - y) ** 2)\n",
    "\n",
    "def find_closest_word(emb, v, similarity=squared_euclidean_sim):\n",
    "    \"\"\"Поиск одного ближайшего слова.\"\"\"\n",
    "    closest_word_value = -1\n",
    "    closest_word = None\n",
    "    for word in emb.vocab.words:\n",
    "        sim = similarity(emb[word], v)\n",
    "        if sim > closest_word_value:\n",
    "            closest_word_value = sim\n",
    "            closest_word = word\n",
    "    return closest_word\n",
    "\n",
    "def find_closest_words(emb, v, k=10, similarity=squared_euclidean_sim):\n",
    "    \"\"\"Поиск k ближайших слов.\n",
    "    \n",
    "    Примечание: функция не очень эффективна, исключительно для демонстрации.\n",
    "    \"\"\"\n",
    "    dists = [(-similarity(emb[word], v), word) for word in emb.vocab.words]\n",
    "    heapq.heapify(dists)\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        top = heapq.heappop(dists)\n",
    "        result.append(top[1])\n",
    "    return result\n",
    "\n",
    "find_closest_words(emb, emb['море'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e4f7a",
   "metadata": {},
   "source": [
    "Попробуем т.н. \"рассуждения по аналогии\". Учитывая специфику набора данных, на которых были обучены вложения слов, пробовать аналогии вроде \"царь - мужчина + женщина ~ царица\" довольно бессмыссленно. Но вот со столицами и странами должно более или менее работать: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a600275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['франция',\n",
       " 'германия',\n",
       " 'бельгия',\n",
       " 'италия',\n",
       " 'великобритания',\n",
       " 'париж',\n",
       " 'австрия',\n",
       " 'испания',\n",
       " 'швеция',\n",
       " 'швейцария']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_words(emb, emb['франция'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0b49d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['париж', 'берлин', 'франция', 'брюссель', 'лондон']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_words(emb, (emb['берлин'] - emb['германия']) + emb['франция'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9e4ca83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['берлин', 'рим', 'мюнхен', 'германия', 'гамбург']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_words(emb, (emb['рим'] - emb['италия']) + emb['германия'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c68703",
   "metadata": {},
   "source": [
    "Действительно, в какой-то мере работает."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bd5ce",
   "metadata": {},
   "source": [
    "Библиотека `natasha` предоставляет специальный класс `NavecEmbedding`, совместимый с фреймворком PyTorch (т.е., реализующий `torch.nn.Module`), и предназначенный для перевода номеров слов в вектора эмбеддингов в рамках нейронных сетей, созданных с помощью этого фреймворка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c716d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from slovnet.model.emb import NavecEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fe3f0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\opt\\miniconda3\\envs\\aim\\lib\\site-packages\\slovnet\\model\\emb.py:46: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  torch.from_numpy(navec.pq.indexes),\n"
     ]
    }
   ],
   "source": [
    "emb_layer = NavecEmbedding(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0004d9",
   "metadata": {},
   "source": [
    "Стандартный подход к обработке текста с помощью нейронных сетей заключается в том, что всем (или почти) словам присваиваются номера, соответствующие строчкам в таблице эмбеддингов. Слой эмбеддингов получает на вход вектор таких идентификаторов слов и на выходе формирует матрицу, составленную из эмбеддиннгов соответствующих слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "105a0bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 300])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([1, 2, 3])\n",
    "emb_layer(input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c557f",
   "metadata": {},
   "source": [
    "Полученный тензор имеет размерность $3 \\times 300$ - это три значения эмбеддингов длиной 300, соответствующие трём входным словам. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384e3fb",
   "metadata": {},
   "source": [
    "Как правило, матрица эмбеддингов содержит еще два специальных значения:\n",
    "\n",
    "- значение для неизвестного слова. Действительно, входное слово может быть редким, может быть написано с опечаткой, поэтому для него может просто не быть эмбеддинга. Все такие слова переводятся в специальное слово `<unk>` и используют одинаковое значение эмбеддинга;\n",
    "- как правило, сеть обрабатывает данные минибатчами. Все последовательности одного минибатча должны быть одинаковой длины (но в разных минибатчах теоретически могут использоваться разные длины), но фактически длина текста может быть различной, поэтому если текст короче, чем длина последовательности минибатча, то он дополняется специальным токеном `<pad>`.\n",
    "\n",
    "Сами значения для неизвестного токена и токена выравнивания (они полезны для преобразования текста в набор идентификаторов), могут быть получены следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d79b0a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.vocab.unk_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e50b5735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250001"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.vocab.pad_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc798c",
   "metadata": {},
   "source": [
    "Номера прочих токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5385592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117429"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.vocab['море']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898258ad",
   "metadata": {},
   "source": [
    "Используя эти данные можно написать функцию, осуществляющую преобразование текста в номера токенов, поддерживаемых слоем эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dbc799c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(emb, text: str, length: int) -> torch.tensor:\n",
    "    \"\"\"Преобразование строки в тензор с номерами токенов.\"\"\"\n",
    "    # Пунктуационные токены (их кодировать не будем)\n",
    "    punct = [',', '.', ';', ':', '-', '...', '!', '?']\n",
    "    # Проведем токенизацию текста\n",
    "    d = natasha.Doc(text)\n",
    "    d.segment(segmenter)\n",
    "    # Для каждого токена, который найдется в словаре эмбеддингов подставим\n",
    "    # его номер, для прочих подставим номер <unk>\n",
    "    tmp = torch.tensor([emb.vocab.get(x.text.lower(), emb.vocab.unk_id)\n",
    "                       for x in d.tokens\n",
    "                       if x.text not in punct][:length])\n",
    "    # Дополним последовательность (спереди) токенами <pad>\n",
    "    return torch.nn.functional.pad(tmp, (length - len(tmp), 0), \"constant\", emb.vocab.pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ce7eb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([250001, 250001, 250001, 250001, 250001, 250001, 248820, 162667, 250000,\n",
       "        111457])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_ids(emb, 'Я помню чюдное мгновенье!', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca598dd",
   "metadata": {},
   "source": [
    "Видно, что все слова были представлены были представлены номерами токенов, слева последовательность дополнена до длины 10 токенами выравнивания (250001), а слово с ошибкой (\"чюдное\") не было найдено в словаре, поэтому для него используется идентификатор неизвестного токена (250000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5807d7f",
   "metadata": {},
   "source": [
    "# 4 Обучение нейронной сети (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2e7451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e1ae0",
   "metadata": {},
   "source": [
    "Преобразуем обучающее и тестовое множества в наборы токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "819c9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack([text_to_ids(emb, x, 50) for x in df_train_small.text], 0)\n",
    "y = torch.unsqueeze(torch.tensor(df_train_small.target, dtype=torch.float32), 1)\n",
    "\n",
    "X_test = torch.stack([text_to_ids(emb, x, 50) for x in df_test.text], 0)\n",
    "y_test = torch.unsqueeze(torch.tensor(df_test.target, dtype=torch.float32), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dce7eb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 33712, 209045, 212839, 119768, 178312, 155642,  59225, 108254,  60824,\n",
       "          70676, 142929, 173082, 205454, 119768, 193013,   1111,   8188,  33712,\n",
       "         178312, 171949, 229673,  42440,  69009,  78526, 240054, 118515, 193605,\n",
       "         115595,  79270, 130886, 250000, 104165, 193522, 209969, 138046, 250000,\n",
       "          35088, 208886, 250000,  75189, 221132, 239317, 102842, 175751, 137130,\n",
       "          16047,  13531,  22480, 129425,  40049]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055edd7",
   "metadata": {},
   "source": [
    "Создадим загрузчики. В данном случае, все данные помещаются в память и хранятся в одном тензоре. Для подобных случаев в PyTorch есть специальный вид `Dataset` - `TensorDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4afab6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X, y), batch_size=8)\n",
    "test_data_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c93754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMClassifier(nn.Module):\n",
    "    \"\"\"Простой классификатор текста.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Специальная обёртка, позволяющая использовать\n",
    "        # эмбеддинги natasha как слой сети PyTorch\n",
    "        self.embedding = NavecEmbedding(emb)\n",
    "        # LSTM-слой.\n",
    "        # Обратите внимание на batch_first=True !\n",
    "        # По умолчанию этот параметр равен False и первая размерность\n",
    "        # интерпретируется как длина последовательности, а не батча - \n",
    "        # если это не поменять, то сеть будет учиться на \"каше\" из данных\n",
    "        self.lstm = nn.LSTM(300,   # размерность элемента последовательности (эмбеддинга)\n",
    "                            30,    # выходная размерность\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(30, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        # Берем только последний выход LSTM ячейки\n",
    "        # (см. вид архитектуры много-к-одному)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9d777fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf709a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0269]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.unsqueeze(text_to_ids(emb, 'Утро туманное...', 5), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0eda2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss=0.0247. Test accuracy 0.9790 ROC_AUC 0.9915\n",
      "Epoch 1: train loss=0.0071. Test accuracy 0.9840 ROC_AUC 0.9953\n",
      "Epoch 2: train loss=0.0066. Test accuracy 0.9870 ROC_AUC 0.9941\n",
      "Epoch 3: train loss=0.0029. Test accuracy 0.9900 ROC_AUC 0.9964\n",
      "Epoch 4: train loss=0.0018. Test accuracy 0.9840 ROC_AUC 0.9977\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for X, y in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_logits = model(X)\n",
    "        loss = criterion(pred_logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_data_loader:\n",
    "            pred_logits = model(X)\n",
    "            preds.append(torch.sigmoid(pred_logits))\n",
    "    preds = torch.cat(preds)\n",
    "    print(f'Epoch {epoch}: train loss={loss.detach().item():.4f}. ' \\\n",
    "          f'Test accuracy {accuracy_score(df_test.target, preds.numpy() > 0.5):.4f} ' \\\n",
    "          f'ROC_AUC {roc_auc_score(df_test.target, preds.numpy()):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
